{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML dataset created and saved to 'ml_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_merge_data(filepath1: str, filepath2: str) -> dict:\n",
    "    \"\"\"Load JSON data from two files and merge them.\"\"\"\n",
    "    with open(filepath1, 'r') as f1, open(filepath2, 'r') as f2:\n",
    "        data1 = json.load(f1)\n",
    "        data2 = json.load(f2)\n",
    "    # Merge the dictionaries, with data2 overwriting data1 in case of conflicts\n",
    "    merged_data = {**data1, **data2}\n",
    "    return merged_data\n",
    "\n",
    "def create_ml_dataframe(data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Create a DataFrame with the specified features for ML analysis.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for artist, attributes in data.items():\n",
    "        # Basic attributes\n",
    "        popularity = attributes.get('popularity', 0)\n",
    "        followers = attributes.get('followers', 0)\n",
    "        \n",
    "        # Collaborator-based features\n",
    "        collaborators = attributes.get('collaborators', {})\n",
    "        num_collaborators = len(collaborators)\n",
    "        \n",
    "        sum_collab_popularity = 0\n",
    "        sum_collab_in_playlist = 0\n",
    "        for collab_name, collab_info in collaborators.items():\n",
    "            # Sum collaborator popularity if available in the dataset\n",
    "            collab_popularity = data.get(collab_name, {}).get('popularity', 0)\n",
    "            sum_collab_popularity += collab_popularity\n",
    "            # Count if collaborator is in the playlist\n",
    "            collab_in_playlist = data.get(collab_name, {}).get('in_playlist', False)\n",
    "            sum_collab_in_playlist += int(collab_in_playlist)\n",
    "        \n",
    "        # Add each artist's record to the dataset\n",
    "        records.append({\n",
    "            'name': artist,\n",
    "            'popularity': popularity,\n",
    "            'followers': followers,\n",
    "            'num_collaborators': num_collaborators,\n",
    "            'sum_collaborators_popularity': sum_collab_popularity,\n",
    "            'sum_collaborators_in_playlist': sum_collab_in_playlist,\n",
    "            'in_playlist': int(attributes.get('in_playlist', False)),\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# Load and merge data from the two JSON files\n",
    "data = load_and_merge_data('11_08_playlist_layer2.json', '10_26_random_2layer.json')\n",
    "\n",
    "# Create the ML DataFrame\n",
    "df_ml = create_ml_dataframe(data)\n",
    "\n",
    "# Save to CSV for ML analysis\n",
    "df_ml.to_csv('ml_dataset.csv', index=False)\n",
    "print(\"ML dataset created and saved to 'ml_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>num_collaborators</th>\n",
       "      <th>sum_collaborators_popularity</th>\n",
       "      <th>sum_collaborators_in_playlist</th>\n",
       "      <th>in_playlist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROSÉ</td>\n",
       "      <td>84</td>\n",
       "      <td>7766971</td>\n",
       "      <td>5</td>\n",
       "      <td>326</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>94</td>\n",
       "      <td>60240128</td>\n",
       "      <td>25</td>\n",
       "      <td>1816</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billie Eilish</td>\n",
       "      <td>96</td>\n",
       "      <td>100730714</td>\n",
       "      <td>13</td>\n",
       "      <td>899</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sabrina Carpenter</td>\n",
       "      <td>95</td>\n",
       "      <td>13640801</td>\n",
       "      <td>20</td>\n",
       "      <td>1325</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sevdaliza</td>\n",
       "      <td>77</td>\n",
       "      <td>704987</td>\n",
       "      <td>15</td>\n",
       "      <td>929</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  popularity  followers  num_collaborators  \\\n",
       "0               ROSÉ          84    7766971                  5   \n",
       "1         Bruno Mars          94   60240128                 25   \n",
       "2      Billie Eilish          96  100730714                 13   \n",
       "3  Sabrina Carpenter          95   13640801                 20   \n",
       "4          Sevdaliza          77     704987                 15   \n",
       "\n",
       "   sum_collaborators_popularity  sum_collaborators_in_playlist  in_playlist  \n",
       "0                           326                              2            1  \n",
       "1                          1816                              3            1  \n",
       "2                           899                              1            1  \n",
       "3                          1325                              3            1  \n",
       "4                           929                              2            1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>followers</th>\n",
       "      <th>num_collaborators</th>\n",
       "      <th>sum_collaborators_popularity</th>\n",
       "      <th>sum_collaborators_in_playlist</th>\n",
       "      <th>in_playlist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3857.000000</td>\n",
       "      <td>3.857000e+03</td>\n",
       "      <td>3857.000000</td>\n",
       "      <td>3857.000000</td>\n",
       "      <td>3857.000000</td>\n",
       "      <td>3857.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.244491</td>\n",
       "      <td>1.230998e+06</td>\n",
       "      <td>18.742546</td>\n",
       "      <td>296.782214</td>\n",
       "      <td>0.387348</td>\n",
       "      <td>0.035261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.011799</td>\n",
       "      <td>6.364674e+06</td>\n",
       "      <td>22.512729</td>\n",
       "      <td>450.290474</td>\n",
       "      <td>0.674310</td>\n",
       "      <td>0.184462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>6.050000e+02</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>9.202000e+03</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.208490e+05</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.244311e+08</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>5687.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity     followers  num_collaborators  \\\n",
       "count  3857.000000  3.857000e+03        3857.000000   \n",
       "mean     39.244491  1.230998e+06          18.742546   \n",
       "std      24.011799  6.364674e+06          22.512729   \n",
       "min       0.000000  0.000000e+00           0.000000   \n",
       "25%      19.000000  6.050000e+02           6.000000   \n",
       "50%      40.000000  9.202000e+03          13.000000   \n",
       "75%      57.000000  2.208490e+05          22.000000   \n",
       "max     100.000000  1.244311e+08         230.000000   \n",
       "\n",
       "       sum_collaborators_popularity  sum_collaborators_in_playlist  \\\n",
       "count                   3857.000000                    3857.000000   \n",
       "mean                     296.782214                       0.387348   \n",
       "std                      450.290474                       0.674310   \n",
       "min                        0.000000                       0.000000   \n",
       "25%                       75.000000                       0.000000   \n",
       "50%                      162.000000                       0.000000   \n",
       "75%                      319.000000                       1.000000   \n",
       "max                     5687.000000                       6.000000   \n",
       "\n",
       "       in_playlist  \n",
       "count  3857.000000  \n",
       "mean      0.035261  \n",
       "std       0.184462  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Cross-Validation F1 Score: 0.53\n",
      "Logistic Regression - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.50\n",
      "  Recall: 0.56\n",
      "  F1 Score: 0.53\n",
      "------------------------------\n",
      "Random Forest - Cross-Validation F1 Score: 0.58\n",
      "Random Forest - Test Set Performance:\n",
      "  Accuracy: 0.97\n",
      "  Precision: 0.60\n",
      "  Recall: 0.56\n",
      "  F1 Score: 0.58\n",
      "------------------------------\n",
      "Support Vector Classifier - Cross-Validation F1 Score: 0.54\n",
      "Support Vector Classifier - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.50\n",
      "  Recall: 0.56\n",
      "  F1 Score: 0.53\n",
      "------------------------------\n",
      "Decision Tree - Cross-Validation F1 Score: 0.47\n",
      "Decision Tree - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.57\n",
      "  Recall: 0.50\n",
      "  F1 Score: 0.53\n",
      "------------------------------\n",
      "Gradient Boosting - Cross-Validation F1 Score: 0.58\n",
      "Gradient Boosting - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.55\n",
      "  Recall: 0.38\n",
      "  F1 Score: 0.44\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:49:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:49:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:49:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:49:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:49:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Cross-Validation F1 Score: 0.56\n",
      "XGBoost - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.56\n",
      "  Recall: 0.62\n",
      "  F1 Score: 0.59\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:49:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 96, number of negative: 2680\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 704\n",
      "[LightGBM] [Info] Number of data points in the train set: 2776, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034582 -> initscore=-3.329224\n",
      "[LightGBM] [Info] Start training from score -3.329224\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 96, number of negative: 2681\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 707\n",
      "[LightGBM] [Info] Number of data points in the train set: 2777, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034570 -> initscore=-3.329597\n",
      "[LightGBM] [Info] Start training from score -3.329597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 96, number of negative: 2681\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 705\n",
      "[LightGBM] [Info] Number of data points in the train set: 2777, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034570 -> initscore=-3.329597\n",
      "[LightGBM] [Info] Start training from score -3.329597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 96, number of negative: 2681\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 704\n",
      "[LightGBM] [Info] Number of data points in the train set: 2777, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034570 -> initscore=-3.329597\n",
      "[LightGBM] [Info] Start training from score -3.329597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 96, number of negative: 2681\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 701\n",
      "[LightGBM] [Info] Number of data points in the train set: 2777, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034570 -> initscore=-3.329597\n",
      "[LightGBM] [Info] Start training from score -3.329597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Cross-Validation F1 Score: 0.58\n",
      "[LightGBM] [Info] Number of positive: 120, number of negative: 3351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 714\n",
      "[LightGBM] [Info] Number of data points in the train set: 3471, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034572 -> initscore=-3.329522\n",
      "[LightGBM] [Info] Start training from score -3.329522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.58\n",
      "  Recall: 0.44\n",
      "  F1 Score: 0.50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network (MLP) - Cross-Validation F1 Score: 0.56\n",
      "Neural Network (MLP) - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.53\n",
      "  Recall: 0.50\n",
      "  F1 Score: 0.52\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('ml_dataset.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df[['popularity', 'followers', 'num_collaborators', 'sum_collaborators_popularity', 'sum_collaborators_in_playlist']]\n",
    "y = df['in_playlist']\n",
    "\n",
    "# Split into training (90%) and testing (10%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Classifier': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "    'LightGBM': LGBMClassifier(),\n",
    "    'Neural Network (MLP)': MLPClassifier(max_iter=300)\n",
    "}\n",
    "\n",
    "# Cross-validation and model evaluation\n",
    "for model_name, model in models.items():\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
    "    print(f\"{model_name} - Cross-Validation F1 Score: {cv_scores.mean():.2f}\")\n",
    "    \n",
    "    # Fit model and make predictions\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} - Test Set Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1 Score: {f1:.2f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 80, 'subsample': 1.0}\n",
      "Best F1 Score: 0.6228338753313475\n",
      "Tuned XGBoost - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.53\n",
      "  Recall: 0.50\n",
      "  F1 Score: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [09:53:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.15, 0.2, 0.5],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'n_estimators': [80, 100, 150],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the tuned model on the test set\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Tuned XGBoost - Test Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "print(f\"  Precision: {precision:.2f}\")\n",
    "print(f\"  Recall: {recall:.2f}\")\n",
    "print(f\"  F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 15, 'n_estimators': 70}\n",
      "Best F1 Score: 0.6030461599245794\n",
      "Tuned Random Forest - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.50\n",
      "  Recall: 0.50\n",
      "  F1 Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 70],          # Number of trees\n",
    "    'max_depth': [None],         # Maximum depth of trees\n",
    "    'min_samples_split': [8, 10, 15],         # Minimum samples to split a node\n",
    "    'min_samples_leaf': [1],           # Minimum samples at a leaf node\n",
    "    'max_features': [None]   # Number of features to consider for splits\n",
    "}\n",
    "\n",
    "# Initialize RandomForest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and the best F1 score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the tuned model on the test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Tuned Random Forest - Test Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "print(f\"  Precision: {precision:.2f}\")\n",
    "print(f\"  Recall: {recall:.2f}\")\n",
    "print(f\"  F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with SMOTE - Test Set Performance:\n",
      "  Accuracy: 0.96\n",
      "  Precision: 0.54\n",
      "  Recall: 0.94\n",
      "  F1 Score: 0.68\n",
      "------------------------------\n",
      "XGBoost with SMOTE - Test Set Performance:\n",
      "  Accuracy: 0.97\n",
      "  Precision: 0.58\n",
      "  Recall: 0.94\n",
      "  F1 Score: 0.71\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:01:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Train Random Forest and XGBoost on the SMOTE-resampled data\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} with SMOTE - Test Set Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1 Score: {f1:.2f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:09:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:09:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:09:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:09:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified K-Fold CV F1 Score: 0.9906680623752017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:09:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Initialize stratified cross-validation\n",
    "stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation on SMOTE-resampled data\n",
    "xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False, learning_rate=0.1, max_depth=5, n_estimators=150)\n",
    "cv_scores = cross_val_score(xgb, X_resampled, y_resampled, cv=stratified_kf, scoring='f1')\n",
    "\n",
    "print(\"Stratified K-Fold CV F1 Score:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final XGBoost Model with SMOTE - Test Set Performance:\n",
      "  Accuracy: 0.97\n",
      "  Precision: 0.58\n",
      "  Recall: 0.94\n",
      "  F1 Score: 0.71\n",
      "Model saved as 'final_xgb_model_with_smote.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zqw\\2024fall\\SI608\\final\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:14:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize XGBoost model with initial parameters\n",
    "final_xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Train the final model on SMOTE-resampled data\n",
    "final_xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the original test set\n",
    "y_pred = final_xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate final model performance on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Final XGBoost Model with SMOTE - Test Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "print(f\"  Precision: {precision:.2f}\")\n",
    "print(f\"  Recall: {recall:.2f}\")\n",
    "print(f\"  F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(final_xgb, \"final_xgb_model_with_smote.pkl\")\n",
    "print(\"Model saved as 'final_xgb_model_with_smote.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
